# GTD-001: Framework Foundation Tests
## Grand Test Document - Comprehensive Real-World Validation

**Document ID:** GTD-001
**Version:** 1.0.0
**Created:** 2024-11-14
**Status:** DRAFT ‚Üí READY FOR EXECUTION
**Test Operator:** Haiku Agent (Cost-Efficient)
**Estimated Cost:** $5-15 (5 projects √ó $1-3 each)

---

## üéØ EXECUTIVE SUMMARY

### Purpose
Validate the **vibe-agency framework** through real-world testing to:
1. Identify gaps between **Design (GAD-001/002)** and **Reality (Code)**
2. Test **PLANNING Phase** (95% implemented) with actual projects
3. Test **CODING Phase** (95% implemented) with real code generation
4. Expose **TESTING/DEPLOYMENT/MAINTENANCE** stubs (10% implemented)
5. Gather **intelligence** for framework refinement

### Critical Question
**"Are we betriebsblind (operationally blind)?"**
‚Üí We've designed GAD-001 and GAD-002 without comprehensive real-world validation.
‚Üí This test will reveal if our designs match reality.

### Test Scope
- ‚úÖ **IN SCOPE:** PLANNING + CODING phases (full E2E)
- ‚ö†Ô∏è **LIMITED SCOPE:** TESTING phase (stub validation only)
- ‚ùå **OUT OF SCOPE:** DEPLOYMENT + MAINTENANCE (acknowledge stubs)

---

## üìä CODE REALITY ASSESSMENT

### What We Found (Deep Dive Analysis)

#### ‚úÖ IMPLEMENTED (95% Functional)

**PLANNING Phase** (`planning_handler.py` - 318 LOC)
- ‚úÖ Research Sub-States (MARKET, TECH, USER, FACT_VALIDATOR)
- ‚úÖ LEAN_CANVAS_VALIDATOR integration
- ‚úÖ VIBE_ALIGNER integration
- ‚úÖ Quality Gates with blocking capability
- ‚úÖ Schema validation
- ‚úÖ Artifact persistence (`research_brief.json`, `lean_canvas_summary.json`, `feature_spec.json`)

**CODING Phase** (`coding_handler.py` - 211 LOC)
- ‚úÖ CODE_GENERATOR 5-task workflow:
  1. Spec Analysis & Validation
  2. Code Generation
  3. Test Generation
  4. Documentation Generation
  5. Quality Assurance & Packaging
- ‚úÖ LLM client integration (`llm_client.py`)
- ‚úÖ Cost tracking and budget enforcement
- ‚úÖ Artifact bundling (`artifact_bundle.zip`)

#### ‚ùå STUBS (10% Functional)

**TESTING Phase** (`testing_handler.py` - 108 LOC)
- ‚ùå Line 7: `TODO (Phase 3): Full implementation`
- ‚ùå Line 65: `status: 'PASSED'` - **AUTO-PASS** (no real tests)
- ‚ùå Stub QA report generation

**DEPLOYMENT Phase** (`deployment_handler.py` - 112 LOC)
- ‚ùå Line 7: `TODO (Phase 3): Full implementation`
- ‚ùå Line 73: `status: 'SUCCESS'` - **FAKE** deployment
- ‚ùå No real deployment logic

**MAINTENANCE Phase** (`maintenance_handler.py` - 106 LOC)
- ‚ùå Line 7: `TODO (Phase 3): Full implementation`
- ‚ùå Line 67: `status: 'HEALTHY'` - **FAKE** monitoring
- ‚ùå No real health checks

### Overall SDLC Status
```
PLANNING:    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 95% ‚úÖ
CODING:      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 95% ‚úÖ
TESTING:     ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 10% ‚ùå (STUB)
DEPLOYMENT:  ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 10% ‚ùå (STUB)
MAINTENANCE: ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 10% ‚ùå (STUB)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
AVERAGE:     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 44% (Functional)
```

---

## üéØ TEST OBJECTIVES

### Primary Objectives
1. **Validate PLANNING Phase** - Does it produce usable, high-quality artifacts?
2. **Validate CODING Phase** - Does CODE_GENERATOR produce working code?
3. **Test E2E Flow** - PLANNING ‚Üí CODING integration works?
4. **Expose Gaps** - What breaks? What's missing? What assumptions are wrong?

### Research Questions
1. **Prompt Quality**
   - Do MARKET_RESEARCHER outputs provide actionable insights?
   - Are VIBE_ALIGNER feature specs implementable?
   - Does scope negotiation work in practice?

2. **Knowledge Base Effectiveness**
   - Are FAE constraints respected?
   - Do TECH_STACK_PATTERNS help or hinder?
   - Do APCE rules prevent bad decisions?

3. **Quality Gates**
   - What's the right FACT_VALIDATOR threshold? (currently 50)
   - Does budget enforcement work?
   - Schema validation: too strict or too loose?

4. **Cost Reality**
   - Average cost per PLANNING phase?
   - Average cost per CODING phase?
   - Where are the most expensive LLM calls?

5. **Error Handling**
   - What happens on LLM failures?
   - Quality gate rejection recovery possible?
   - Schema validation errors easy to debug?

---

## üìã TEST PROJECTS (Vaisnava/Hare Krishna Domain)

### Project Selection Criteria
- **Domain:** Vaisnava/Hare Krishna spirituality (user's domain expertise)
- **Variety:** Different types (Web, Mobile, CLI, API, Desktop)
- **Complexity:** Range from Simple ‚Üí Medium ‚Üí Complex
- **Real-World:** Projects that could actually be built and used

---

### PROJECT 1: Srila Prabhupada Quote of the Day

**Category:** Simple CLI Tool
**Estimated Complexity:** Low
**Estimated Cost:** $1-2

**User Input (Raw):**
```
I want a simple command-line tool that shows a random quote from Srila Prabhupada's books each day.
The tool should:
- Display a new quote every day at startup
- Show the source (which book, chapter)
- Allow filtering by book (e.g., only Bhagavad-Gita quotes)
- Work offline once quotes are downloaded
- Be written in Python

Target users: Devotees who work on terminal/command line
```

**Test Focus:**
- Can MARKET_RESEARCHER find similar tools (quote apps)?
- Does VIBE_ALIGNER correctly identify scope (offline, CLI, simple)?
- Does CODE_GENERATOR produce working Python CLI code?
- Quality of generated tests?

**Success Criteria:**
- ‚úÖ `feature_spec.json` includes offline capability
- ‚úÖ Generated code is runnable Python CLI
- ‚úÖ Tests cover main functionality
- ‚úÖ Documentation explains how to add new quotes

---

### PROJECT 2: Temple Companion Mobile App

**Category:** Medium Mobile App
**Estimated Complexity:** Medium
**Estimated Cost:** $2-3

**User Input (Raw):**
```
A mobile app for devotees to help them during temple visits. Features:
- Daily darshan times for major ISKCON temples worldwide
- Prasadam menu of the day (if temple provides)
- Festival calendar (Ekadashi, appearance days, etc.)
- Kirtan schedule
- Find nearby ISKCON temples (location-based)
- Offline mode for downloaded temple data

Target: iOS and Android users visiting ISKCON temples
Should be free to use, data can be community-contributed
```

**Test Focus:**
- Can MARKET_RESEARCHER find competitor apps (temple finders, Hindu calendars)?
- Does VIBE_ALIGNER identify community-contribution complexity?
- Does TECH_RESEARCHER recommend appropriate mobile framework?
- FAE constraints: Does it flag offline data sync challenges?

**Success Criteria:**
- ‚úÖ Market research identifies 3+ competitor apps
- ‚úÖ Tech stack recommendation (React Native vs Flutter vs Native)
- ‚úÖ Feature spec includes data sync architecture
- ‚úÖ Cost estimate is realistic for MVP

---

### PROJECT 3: Hare Krishna Japa Counter Web App

**Category:** Simple Web App
**Estimated Complexity:** Low-Medium
**Estimated Cost:** $1-2

**User Input (Raw):**
```
A web application to help devotees track their japa (chanting) rounds.
Features:
- Counter for daily rounds (16 rounds minimum goal)
- Track time per round
- Monthly statistics and streaks
- Reminders/notifications for chanting goals
- Simple, distraction-free UI
- Optional: Social features (see friends' progress)

Target: Web-first (mobile responsive), free to use
Tech preference: Modern web stack, fast loading
```

**Test Focus:**
- Does MARKET_RESEARCHER identify similar apps (habit trackers, meditation timers)?
- Does VIBE_ALIGNER scope-negotiate social features (complexity spike)?
- Does CODE_GENERATOR produce clean, modern web code?
- Quality of generated component tests?

**Success Criteria:**
- ‚úÖ Scope negotiation flags social features as Phase 2
- ‚úÖ Tech stack is modern (React/Vue/Svelte + backend)
- ‚úÖ Generated code includes responsive design
- ‚úÖ Tests cover counter logic thoroughly

---

### PROJECT 4: Bhagavad Gita API Service

**Category:** Medium API/Backend
**Estimated Complexity:** Medium
**Estimated Cost:** $2-3

**User Input (Raw):**
```
RESTful API service providing Bhagavad Gita verses and translations.
Features:
- GET verse by chapter and number
- Search verses by keyword
- Multiple translations (Prabhupada, others)
- Word-by-word Sanskrit breakdowns
- Audio verse recitation URLs
- Rate limiting and API key management
- Comprehensive documentation

Target: Developers building Gita apps/websites
Licensing: Data is public domain, API usage is free with attribution
```

**Test Focus:**
- Does MARKET_RESEARCHER find existing Gita APIs?
- Does TECH_RESEARCHER evaluate data sources and licensing?
- Does VIBE_ALIGNER identify rate limiting as critical?
- CODE_GENERATOR: API design quality?

**Success Criteria:**
- ‚úÖ Market research finds 2+ existing Gita APIs
- ‚úÖ Feature spec includes data source validation
- ‚úÖ Generated API follows REST best practices
- ‚úÖ Documentation is comprehensive

---

### PROJECT 5: Vaishnava Book Library Manager

**Category:** Complex Desktop App
**Estimated Complexity:** High
**Estimated Cost:** $3-5

**User Input (Raw):**
```
Desktop application for managing a personal library of Vaishnava literature.
Features:
- Catalog physical books (title, author, ISBN, location in library)
- Track borrowed books (who borrowed, when, reminders)
- Reading lists and progress tracking
- Notes and highlights per book
- Search and filter (by author, topic, language)
- Export library catalog
- Barcode scanner integration (optional)
- Multi-user support (family library)

Target: Windows/Mac/Linux users with large Vaishnava book collections
Preference: Electron or native app, local-first data storage
```

**Test Focus:**
- Can MARKET_RESEARCHER find similar library management software?
- Does VIBE_ALIGNER scope complex features (multi-user, barcode)?
- Does TECH_RESEARCHER evaluate local-first vs cloud storage?
- CODE_GENERATOR: Can it handle complex desktop app architecture?

**Success Criteria:**
- ‚úÖ Scope negotiation defers barcode/multi-user to Phase 2
- ‚úÖ Tech stack decision is well-reasoned (Electron vs Tauri vs Native)
- ‚úÖ Feature spec includes data backup strategy
- ‚úÖ Generated code has proper desktop app structure

---

## üìè TEST CRITERIA

### Artifact Quality Metrics

#### 1. `research_brief.json` (PLANNING Phase - Research)
**Required Fields:**
- `market_research.competitors` - List of 2+ competitor products
- `market_research.market_size_estimate` - With source/citation
- `tech_research.recommended_stack` - Tech stack recommendation
- `tech_research.feasibility_score` - Numeric score (0-100)
- `user_research.target_personas` - At least 1 persona
- `fact_validation.quality_score` - Score >= 50 (passing threshold)

**Success Criteria:**
- ‚úÖ All competitors have URLs/sources
- ‚úÖ Market size has citation (not "AI estimated")
- ‚úÖ Tech stack matches project type (CLI ‚Üí Python, Web ‚Üí React/Vue, etc.)
- ‚úÖ Feasibility score is realistic (not 100% for complex projects)
- ‚úÖ Quality score >= 50 (otherwise gate blocks)

#### 2. `lean_canvas_summary.json` (PLANNING Phase - Business)
**Required Fields:**
- `problem` - Clear problem statement
- `solution` - High-level solution
- `unique_value_proposition` - What makes it unique
- `unfair_advantage` - Competitive moat (can be "none" for simple projects)
- `revenue_streams` - Even if "free"/"donation-based"
- `cost_structure` - Development + operational costs

**Success Criteria:**
- ‚úÖ Problem aligns with user input
- ‚úÖ UVP is not generic ("easy to use", "fast")
- ‚úÖ Cost structure is realistic

#### 3. `feature_spec.json` (PLANNING Phase - Features)
**Required Fields:**
- `core_features` - List of MVP features
- `phase_2_features` - Nice-to-haves deferred
- `technical_requirements` - Performance, scalability, security
- `constraints` - FAE/APCE violations flagged
- `code_gen_spec_ref` - Reference for CODE_GENERATOR

**Success Criteria:**
- ‚úÖ Core features are scoped to MVP (not kitchen sink)
- ‚úÖ Phase 2 features show scope negotiation worked
- ‚úÖ Technical requirements are specific (not "should be fast")
- ‚úÖ FAE violations are documented (if any)

#### 4. `artifact_bundle` (CODING Phase)
**Required Files:**
- Source code files (`.py`, `.js`, `.tsx`, etc.)
- Test files (`test_*.py`, `*.test.js`, etc.)
- Documentation (`README.md`, API docs)
- Build configuration (`package.json`, `requirements.txt`, etc.)

**Success Criteria:**
- ‚úÖ Code is syntactically valid (passes linting)
- ‚úÖ Tests are present and runnable
- ‚úÖ README explains how to run the project
- ‚úÖ Dependencies are reasonable (not 100+ packages for simple app)

### Quality Gates Validation

#### FACT_VALIDATOR Gate
**Threshold:** quality_score >= 50

**Test Scenarios:**
1. **Should PASS:** Well-researched project with sources
2. **Should BLOCK:** Project with no competitor research (score < 50)
3. **Edge Case:** Project with 1 competitor (marginal score ~45-55)

**Validation:**
- ‚úÖ Gate correctly blocks when score < 50
- ‚úÖ Error message is actionable
- ‚úÖ Can retry after fixing issues

#### Budget Gate
**Threshold:** cost_so_far < manifest.budget_limit

**Test Scenarios:**
1. **Should PASS:** Normal project ($1-3 cost)
2. **Should BLOCK:** Set budget_limit = $0.50 artificially
3. **Edge Case:** Budget exactly at limit

**Validation:**
- ‚úÖ Gate correctly blocks when budget exceeded
- ‚úÖ Partial results are saved before blocking
- ‚úÖ Error shows remaining budget

### Cost Metrics

**Target Costs (Estimates):**
- PLANNING Phase: $0.50 - $2.00 per project
  - MARKET_RESEARCHER: $0.20 - $0.50
  - TECH_RESEARCHER: $0.20 - $0.50
  - USER_RESEARCHER: $0.10 - $0.30
  - FACT_VALIDATOR: $0.10 - $0.30
  - LEAN_CANVAS_VALIDATOR: $0.10 - $0.20
  - VIBE_ALIGNER: $0.20 - $0.50

- CODING Phase: $0.50 - $3.00 per project
  - Spec Analysis: $0.10 - $0.30
  - Code Generation: $0.20 - $1.00 (largest cost)
  - Test Generation: $0.10 - $0.50
  - Documentation: $0.10 - $0.30
  - QA Packaging: $0.10 - $0.30

**Measurement:**
- ‚úÖ Track actual costs per agent
- ‚úÖ Identify most expensive operations
- ‚úÖ Compare estimate vs actual

---

## üöÄ TEST EXECUTION PLAN (For Haiku Agent)

### Prerequisites

1. **Environment Setup**
   ```bash
   cd /home/user/vibe-research

   # Create test workspace
   mkdir -p workspaces/gtd_test_001

   # Set API keys (must be configured)
   # Required: ANTHROPIC_API_KEY, GOOGLE_SEARCH_API_KEY, GOOGLE_SEARCH_ENGINE_ID
   # Optional: GITHUB_TOKEN (for library checks)
   ```

2. **Budget Configuration**
   ```python
   # In each project manifest
   {
     "budget_limit_usd": 5.00,  # $5 max per project
     "cost_tracking_enabled": true
   }
   ```

3. **Test Data Preparation**
   - Save 5 project inputs as `project_<N>_input.txt`
   - Each contains raw user input (from GTD projects above)

### Execution Sequence

#### Phase 1: Individual Project Tests (Projects 1-5)

**For Each Project:**

```bash
# Step 1: Create project workspace
PROJECT_ID="gtd_prabhupada_quotes"  # Project 1
python -m agency_os.00_system.orchestrator.core_orchestrator init $PROJECT_ID

# Step 2: Run PLANNING phase
python -m agency_os.00_system.orchestrator.core_orchestrator run $PROJECT_ID --phases PLANNING

# Step 3: Validate PLANNING artifacts
python scripts/validate_artifacts.py $PROJECT_ID --phase PLANNING

# Step 4: Run CODING phase
python -m agency_os.00_system.orchestrator.core_orchestrator run $PROJECT_ID --phases CODING

# Step 5: Validate CODING artifacts
python scripts/validate_artifacts.py $PROJECT_ID --phase CODING

# Step 6: Extract code and attempt to run
python scripts/extract_and_test_code.py $PROJECT_ID
```

**Collect Metrics:**
```json
{
  "project_id": "gtd_prabhupada_quotes",
  "phase": "PLANNING",
  "duration_seconds": 120,
  "cost_usd": 1.23,
  "artifacts_created": ["research_brief.json", "lean_canvas_summary.json", "feature_spec.json"],
  "quality_gates_passed": true,
  "fact_validator_score": 67,
  "issues": []
}
```

#### Phase 2: E2E Integration Test

**Test Full PLANNING ‚Üí CODING ‚Üí TESTING (stub) flow:**

```bash
# Run all phases for one project
PROJECT_ID="gtd_temple_companion"
python -m agency_os.00_system.orchestrator.core_orchestrator run $PROJECT_ID --phases ALL

# Validate complete flow
python scripts/validate_e2e_flow.py $PROJECT_ID
```

**Expected Behavior:**
- ‚úÖ PLANNING completes successfully
- ‚úÖ CODING receives feature_spec and generates code
- ‚ö†Ô∏è TESTING runs but produces STUB qa_report (acknowledged)
- ‚ùå DEPLOYMENT/MAINTENANCE should be skipped or stubbed (acknowledged)

#### Phase 3: Failure Mode Testing

**Test 1: Quality Gate Rejection**
```bash
# Create project with intentionally poor input
PROJECT_ID="gtd_test_bad_input"
echo "Build me a thing" > workspaces/gtd_test_bad_input/input.txt

python -m agency_os.00_system.orchestrator.core_orchestrator run $PROJECT_ID --phases PLANNING

# Expected: FACT_VALIDATOR blocks with score < 50
```

**Test 2: Budget Exhaustion**
```bash
# Set artificially low budget
PROJECT_ID="gtd_test_budget"
# Modify manifest: budget_limit_usd = 0.10

python -m agency_os.00_system.orchestrator.core_orchestrator run $PROJECT_ID --phases PLANNING

# Expected: Stops mid-execution with budget error
```

**Test 3: Schema Validation Failure**
```bash
# Corrupt an artifact
PROJECT_ID="gtd_test_schema"
# After PLANNING, manually edit research_brief.json with invalid schema

python -m agency_os.00_system.orchestrator.core_orchestrator run $PROJECT_ID --phases CODING

# Expected: Schema validation error with clear message
```

### Data Collection

**For Each Test:**
1. Save all console logs ‚Üí `logs/gtd_<project_id>_<timestamp>.log`
2. Save all artifacts ‚Üí `workspaces/<project_id>/artifacts/`
3. Export metrics ‚Üí `test_results/gtd_metrics.json`
4. Screenshot any errors ‚Üí `test_results/errors/`

**Consolidated Report:**
```json
{
  "test_run_id": "GTD-001-20241114",
  "timestamp": "2024-11-14T19:30:00Z",
  "total_projects": 5,
  "total_cost_usd": 8.47,
  "avg_cost_per_project": 1.69,
  "success_rate": {
    "planning_phase": "5/5 (100%)",
    "coding_phase": "4/5 (80%)",
    "e2e_flow": "4/5 (80%)"
  },
  "quality_gate_stats": {
    "fact_validator_blocks": 1,
    "budget_blocks": 1,
    "schema_errors": 0
  },
  "findings": [
    "Issue #1: TECH_RESEARCHER recommended Express.js for CLI tool (mismatch)",
    "Issue #2: CODE_GENERATOR hallucinated import for non-existent library",
    "Issue #3: Feature spec included 'social features' despite scope negotiation"
  ]
}
```

---

## üìê EXPECTED vs. ACTUAL FRAMEWORK

### Gap Analysis Template

For each test project, document gaps:

#### Design Expectation (GAD-001/002)
> "MARKET_RESEARCHER should identify 3-5 competitors with sources"

#### Code Reality
> MARKET_RESEARCHER found 2 competitors, but one URL was broken (404)

#### Gap Severity
- üü¢ **Minor:** Cosmetic issue, doesn't affect functionality
- üü° **Moderate:** Works but suboptimal, needs improvement
- üî¥ **Critical:** Blocks usage or produces incorrect results

#### Recommended Action
- [ ] Update GAD documentation
- [ ] Fix code implementation
- [ ] Add validation/quality gate
- [ ] Document as known limitation

### Common Gaps to Watch For

1. **Knowledge Base Gaps**
   - ‚ùì Are FAE constraints too strict for non-commercial projects?
   - ‚ùì Do TECH_STACK_PATTERNS cover all project types (CLI, Mobile, Desktop)?
   - ‚ùì Are APCE rules culturally sensitive (Vaisnava projects)?

2. **Prompt Quality Gaps**
   - ‚ùì Do agents hallucinate URLs/sources?
   - ‚ùì Are cost estimates realistic?
   - ‚ùì Do feature specs include non-functional requirements?

3. **Error Handling Gaps**
   - ‚ùì What happens if Google Search API is rate-limited?
   - ‚ùì Can workflow recover from transient LLM failures?
   - ‚ùì Are error messages actionable for non-technical users?

4. **Workflow Gaps**
   - ‚ùì Can PLANNING artifacts be manually edited and re-validated?
   - ‚ùì Is CODING phase resumable if interrupted?
   - ‚ùì Can quality gate thresholds be customized per project?

---

## üî¨ RESEARCH QUESTIONS & HYPOTHESES

### Hypothesis 1: Prompt Quality
**H1:** MARKET_RESEARCHER will find relevant competitors for all 5 Vaisnava projects.

**Test:** Check `research_brief.json` for each project
**Success Metric:** >= 2 competitors with valid URLs
**Data to Collect:**
- Number of competitors found
- Relevance score (manual review)
- URL validity (automated check)

**Possible Outcomes:**
- ‚úÖ **Confirmed:** Found 2+ relevant competitors for all projects
- ‚ö†Ô∏è **Partial:** Found competitors but some irrelevant (e.g., generic meditation apps for Japa counter)
- ‚ùå **Rejected:** Could not find Vaisnava-specific competitors (too niche)

---

### Hypothesis 2: Scope Negotiation
**H2:** VIBE_ALIGNER will defer complex features to Phase 2 (e.g., social features, barcode scanning).

**Test:** Compare user input vs `feature_spec.json` core_features
**Success Metric:** Complex features in `phase_2_features`, not `core_features`
**Data to Collect:**
- Features mentioned in input
- Features in core vs phase_2
- Scope negotiation rationale

**Possible Outcomes:**
- ‚úÖ **Confirmed:** All complex features correctly deferred
- ‚ö†Ô∏è **Partial:** Some complex features in core (scope creep)
- ‚ùå **Rejected:** No scope negotiation (everything in core)

---

### Hypothesis 3: Code Quality
**H3:** CODE_GENERATOR will produce syntactically valid, runnable code for simple projects (CLI, Web).

**Test:** Extract code, run linters, attempt execution
**Success Metric:** Code passes linting + runs without import errors
**Data to Collect:**
- Linting errors count
- Import errors count
- Test execution success rate

**Possible Outcomes:**
- ‚úÖ **Confirmed:** Code is valid and runnable
- ‚ö†Ô∏è **Partial:** Code is valid but missing dependencies or config
- ‚ùå **Rejected:** Code has syntax errors or hallucinated imports

---

### Hypothesis 4: Cost Efficiency
**H4:** PLANNING + CODING will cost < $3 per simple project, < $5 per complex project.

**Test:** Track LLM costs via llm_client.py
**Success Metric:** Actual costs within estimates
**Data to Collect:**
- Cost per agent/task
- Token usage breakdown
- Most expensive operations

**Possible Outcomes:**
- ‚úÖ **Confirmed:** Costs within budget
- ‚ö†Ô∏è **Partial:** Costs 2x estimate (needs optimization)
- ‚ùå **Rejected:** Costs 5x+ estimate (model too expensive)

---

### Hypothesis 5: Quality Gates
**H5:** FACT_VALIDATOR threshold of 50 is appropriate (not too strict, not too lenient).

**Test:** Review blocked vs passed projects
**Success Metric:** Gate blocks <20% of valid projects, passes >95% of quality projects
**Data to Collect:**
- Projects blocked (score < 50)
- False positives (good project blocked)
- False negatives (bad project passed)

**Possible Outcomes:**
- ‚úÖ **Confirmed:** Threshold is balanced
- ‚ö†Ô∏è **Too Strict:** Blocks valid projects (lower to 40?)
- ‚ö†Ô∏è **Too Lenient:** Passes low-quality projects (raise to 60?)

---

## üéØ OPERATOR INSTRUCTIONS (For Haiku Agent)

### Your Role
You are the **Test Operator**. Your job is to:
1. Execute all 5 project tests systematically
2. Collect ALL data (logs, artifacts, metrics, errors)
3. Document EVERY finding (gaps, issues, surprises)
4. Stay objective - report reality, not what you wish was true

### Execution Guidelines

**DO:**
- ‚úÖ Follow test sequence exactly as written
- ‚úÖ Save all logs and artifacts (we'll analyze later)
- ‚úÖ Document unexpected behavior immediately
- ‚úÖ Track costs per project accurately
- ‚úÖ Note timestamps for duration tracking

**DON'T:**
- ‚ùå Skip steps or projects ("this looks similar to Project 2")
- ‚ùå Fix code manually (we want to see raw output)
- ‚ùå Retry failed tests without documenting the failure
- ‚ùå Optimize prompts mid-test (test "as is")
- ‚ùå Editorialize findings (just facts)

### Reporting Format

**For Each Project, Report:**

```markdown
## Project X: [Name]

### Execution Summary
- Start Time: 2024-11-14 19:45:00 UTC
- End Time: 2024-11-14 19:52:30 UTC
- Duration: 7m 30s
- Total Cost: $1.87
- Status: ‚úÖ SUCCESS / ‚ö†Ô∏è PARTIAL / ‚ùå FAILURE

### Phase: PLANNING
- Duration: 4m 15s
- Cost: $0.92
- Artifacts Created: research_brief.json, lean_canvas_summary.json, feature_spec.json
- Quality Score: 67/100 ‚úÖ PASSED gate
- Issues:
  1. MARKET_RESEARCHER found 1 competitor (expected 2+)
  2. Tech stack recommendation was generic ("modern web stack")

### Phase: CODING
- Duration: 3m 15s
- Cost: $0.95
- Artifacts Created: artifact_bundle.zip (12 files)
- Linting: ‚úÖ 0 errors, 3 warnings
- Test Execution: ‚úÖ 8/10 tests passed
- Issues:
  1. Missing dependency: 'requests' not in requirements.txt
  2. Test file had incorrect import path

### Findings
**Gaps Identified:**
1. [MODERATE] TECH_RESEARCHER didn't check if recommended library is actively maintained
2. [MINOR] CODE_GENERATOR added TODO comments in production code

**Positive Observations:**
1. Feature spec correctly deferred social features to Phase 2
2. Generated README was comprehensive and clear

**Questions Raised:**
1. Should FAE constraints flag libraries with <100 stars on GitHub?
2. Is 85% test coverage target too high for CLI tools?

### Raw Data
- Logs: `/logs/gtd_prabhupada_quotes_20241114_194500.log`
- Artifacts: `/workspaces/gtd_prabhupada_quotes/artifacts/`
- Metrics: `/test_results/project_1_metrics.json`
```

### Final Deliverable

**Consolidated Test Report:**
```markdown
# GTD-001 Test Results

## Executive Summary
- Projects Tested: 5/5
- Total Cost: $8.47
- Total Duration: 38m 15s
- Success Rate: 80% (4/5 completed successfully)

## Key Findings
1. **Critical:** CODE_GENERATOR hallucinated library imports (2/5 projects)
2. **Moderate:** MARKET_RESEARCHER found <2 competitors for niche projects (2/5)
3. **Minor:** Feature specs lacked non-functional requirements (3/5)

## Recommendations
1. Add import validation gate before CODING phase
2. Lower competitor threshold for niche domains (2 ‚Üí 1)
3. Update VIBE_ALIGNER prompt to include NFRs

## Gap Analysis
[Detailed per-project findings]

## Cost Breakdown
[Per-agent cost analysis]

## Next Steps
1. Address Critical findings before GAD-003
2. Refine MARKET_RESEARCHER prompt
3. Add import validation script
```

---

## üìä SUCCESS CRITERIA FOR THIS GTD

This test plan is **READY FOR EXECUTION** when:

- ‚úÖ All 5 projects have detailed specifications (user input)
- ‚úÖ Test criteria are measurable and objective
- ‚úÖ Execution plan is step-by-step (no ambiguity)
- ‚úÖ Data collection is comprehensive (logs, artifacts, metrics)
- ‚úÖ Operator can execute without asking clarifying questions

This test plan is **SUCCESSFUL** when:

- ‚úÖ All 5 projects executed (success or documented failure)
- ‚úÖ All data collected and consolidated
- ‚úÖ Gaps documented with severity and recommended actions
- ‚úÖ Cost tracking validated actual vs estimated
- ‚úÖ Findings inform GAD-003 refinement

---

## üöß RESEARCH FRAMEWORK CAVEAT

**IMPORTANT:** The Research Framework (`01_research_framework` ‚Üí `01_planning_framework/agents/research`) is **PROTOTYPE STATUS**.

**What This Means:**
- ‚úÖ Core logic is implemented
- ‚ö†Ô∏è Prompts may need refinement
- ‚ö†Ô∏è Knowledge bases may have gaps
- ‚ö†Ô∏è Edge cases may not be handled

**Test Accordingly:**
- Document all prompt quality issues
- Note knowledge base gaps (e.g., missing FAE rules)
- Identify edge cases that break workflow
- Suggest refinements for each agent

**This is Intelligence Gathering, Not QA:**
- Goal: Learn what works and what doesn't
- Not Goal: Prove the framework is perfect

---

## üîÑ TEST ITERATION PLAN

### Round 1: Foundation (This Document)
- Execute 5 projects
- Identify critical gaps
- Validate PLANNING + CODING phases

### Round 2: Refinement (After Fixes)
- Re-test failed projects
- Add 2-3 new test projects
- Validate fixes don't break existing functionality

### Round 3: Stress Test (Optional)
- 10+ projects across different domains
- Budget constraints testing
- Concurrent execution testing

---

## ‚úÖ READY FOR EXECUTION

**This GTD is APPROVED for execution by Haiku Agent.**

**Estimated Timeline:**
- Setup: 30 minutes
- Execution: 2-3 hours (5 projects √ó 20-30 min each)
- Analysis: 1-2 hours
- Report: 1 hour
- **Total: 4-6 hours**

**Estimated Cost:**
- Projects: $5-10
- Operator time: Haiku model (cost-efficient)
- **Total: $5-15**

**Next Steps:**
1. User approves GTD-001
2. Haiku Agent executes test plan
3. Findings inform framework refinement
4. Decide if GAD-003 is premature

---

**Document Status:** ‚úÖ READY
**Approval Required:** User Confirmation
**Execution Clearance:** Pending

---

END OF GTD-001
