# üî¨ RESEARCH-PL√ÑNE F√úR FEHLENDE FRAMEWORKS

---

## RESEARCH #1: CODE GENERATION FRAMEWORK

**Ziel:** Systematisches Framework f√ºr architecture.json ‚Üí lauff√§higer Code

**Research-Fragen:**

### 1. Constraints (FAE-√§hnlich)
- Welche technischen Constraints gelten f√ºr Code-Generierung?
- Was kann AI NICHT generieren (z.B. Legacy-Integration)?
- Welche Code-Muster f√ºhren zu Halluzinationen?
- Welche Sprach-/Framework-Kombinationen sind problematisch?

### 2. Dependencies (FDG-√§hnlich)
- Welche Inputs braucht Code-Gen? (architecture.json + was noch?)
- Welche Context-Informationen sind kritisch? (existing code, libs, APIs)
- Welche Artefakte werden erzeugt? (source code, tests, docs)
- Welche Abh√§ngigkeiten zwischen Code-Komponenten?

### 3. Quality Rules (APCE-√§hnlich)
- Wie misst man Code-Qualit√§t automatisch?
- Welche Metriken f√ºr "good enough for v1.0"?
- Wann ist Code "ready for QA"?
- Wie priorisiert man Code-Generierung? (Core first, Extensions second)

**Output-Format:**
- `CODE_GEN_constraints.yaml`
- `CODE_GEN_dependencies.yaml`
- `CODE_GEN_quality_rules.yaml`

---

## RESEARCH #2: QA/TESTING FRAMEWORK

**Ziel:** Systematisches Framework f√ºr Code ‚Üí Validated Product

**Research-Fragen:**

### 1. Constraints (FAE-√§hnlich)
- Was kann NICHT automatisch getestet werden?
- Welche Test-Typen sind f√ºr v1.0 unrealistisch? (Load, Security, Penetration)
- Welche Testing-Tools haben welche Limits?
- Wann MUSS ein Mensch testen? (HITL-Constraints)

### 2. Dependencies (FDG-√§hnlich)
- Welche Test-Typen brauchen welche Inputs?
- Test-Pyramide: Unit ‚Üí Integration ‚Üí E2E (Dependencies?)
- Welche Tests m√ºssen in welcher Reihenfolge laufen?
- Was braucht QA vom Code-Gen-Framework?

### 3. Quality Rules (APCE-√§hnlich)
- Welche Test-Coverage f√ºr v1.0 ausreichend?
- Wie priorisiert man Tests? (Critical Path zuerst)
- Wann ist "QA approved"? (Pass-Kriterien)
- Wie handled man flaky tests?

**Output-Format:**
- `QA_constraints.yaml`
- `QA_dependencies.yaml`
- `QA_quality_rules.yaml`

---

## RESEARCH #3: DEPLOYMENT FRAMEWORK

**Ziel:** Systematisches Framework f√ºr Validated Code ‚Üí Production

**Research-Fragen:**

### 1. Constraints (FAE-√§hnlich)
- Welche Deployment-Strategien sind f√ºr v1.0 unrealistisch? (Blue-Green, Canary)
- Welche Infrastruktur-Komplexit√§t ist zu hoch?
- Was kann NICHT automatisiert werden? (DNS, Certs, Compliance)
- Welche Cloud-Provider haben welche Limits?

### 2. Dependencies (FDG-√§hnlich)
- Was braucht Deployment vom QA-Framework? (approval report)
- Welche Deployment-Steps in welcher Reihenfolge?
- Rollback-Dependencies?
- Monitoring/Observability-Requirements?

### 3. Quality Rules (APCE-√§hnlich)
- Wann ist "deployment successful"?
- Welche Health-Checks f√ºr v1.0?
- Wie priorisiert man Deployment-Schritte?
- Wann Rollback triggern?

**Output-Format:**
- `DEPLOY_constraints.yaml`
- `DEPLOY_dependencies.yaml`
- `DEPLOY_quality_rules.yaml`

---

## RESEARCH #4: MAINTENANCE/BUG-TRIAGE FRAMEWORK

**Ziel:** Systematisches Framework f√ºr Production Issues ‚Üí Fixes

**Research-Fragen:**

### 1. Constraints (FAE-√§hnlich)
- Welche Bugs k√∂nnen NICHT automatisch getriagt werden?
- Welche Bug-Typen brauchen HITL? (Security, Data Loss)
- Was kann AI NICHT fixen? (Architecture changes, DB migrations)
- Welche Bug-Severity-Levels existieren?

### 2. Dependencies (FDG-√§hnlich)
- Was braucht Bug-Triage? (logs, monitoring, user reports)
- Wie wird Bug ‚Üí Feature Request ‚Üí Planning Loop geschlossen?
- Welche Bug-Types brauchen welche Fix-Workflows?
- Hotfix vs. Regular Fix Dependencies?

### 3. Quality Rules (APCE-√§hnlich)
- Wie priorisiert man Bugs? (Severity √ó Impact)
- Wann ist ein Bug "fixed"? (Tests passed + deployed)
- SLA-Targets f√ºr Bug-Response? (Critical: 1h, Major: 1d, Minor: 1w)
- Wann eskaliert man zum Menschen?

**Output-Format:**
- `MAINTENANCE_constraints.yaml`
- `MAINTENANCE_dependencies.yaml`
- `MAINTENANCE_triage_rules.yaml`

---

## RESEARCH #5: ORCHESTRATION META-FRAMEWORK

**Ziel:** Wie verbindet man alle Frameworks?

**Research-Fragen:**

### 1. Workflow Design
- Wie modelliert man SDLC als State Machine?
- Welche States? (PLANNING ‚Üí CODING ‚Üí TESTING ‚Üí DEPLOYED ‚Üí MAINTENANCE)
- Welche Transitions? (QA approved ‚Üí Trigger Deploy)
- Error-Handling? (Test failed ‚Üí Back to Coding)

### 2. Data Contracts
- JSON-Schema f√ºr project_manifest.json finalisieren
- JSON-Schemas f√ºr alle Artefakte (code_gen_spec, test_plan, qa_report, deploy_receipt, bug_report)
- Versionierung-Strategy?
- Schema-Evolution?

### 3. Orchestrator Selection
- Temporal vs. Prefect vs. GitHub Actions vs. Custom?
- Trade-offs f√ºr v1.0?
- Welche Features sind kritisch? (Durable execution, HITL-support, Observability)
- Setup-Komplexit√§t vs. Capabilities?

**Output-Format:**
- `ORCHESTRATION_workflow_design.yaml`
- `ORCHESTRATION_data_contracts.yaml`
- `ORCHESTRATION_technology_comparison.yaml`

---

## üìã ZUSAMMENFASSUNG

**5 Research-Pl√§ne:**
1. Code Generation (3 YAMLs)
2. QA/Testing (3 YAMLs)
3. Deployment (3 YAMLs)
4. Maintenance (3 YAMLs)
5. Orchestration (3 YAMLs)

**Total Output:** 15 YAMLs + Knowledge Base

**Execution:** Du leitest diese an externes Research-Team weiter.

**Sobald zur√ºck:** Wir bauen die Frameworks (wie VIBE+GENESIS, aber f√ºr Code/QA/Deploy/Maintenance).

---

**FERTIG. Keine Fragen. Copy-paste ready.** üöÄ