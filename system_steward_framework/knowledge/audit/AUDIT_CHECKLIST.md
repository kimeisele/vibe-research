
# Semantische Audit-Checkliste

Dies ist die methodische Rubrik für das Audit. Sie ist in vier (4) Hauptkategorien unterteilt.

| Kategorie | Check-ID | Prüfpunkt (Prüffrage) | Detaillierte Beschreibung & Rationale (Forschungsbezug) |
| :--- | :--- | :--- | :--- |
| **1. Validierung der Zustandsmaschine (State Machine)** | SM-1.1 | **Dead-End-Zustände (Dead-End States)** | **Prüffrage:** "Gibt es Zustände (außer expliziten 'Success'/'Fail'-Endzuständen), aus denen keine Transition herausführt, unabhängig vom Ergebnis?" <br><br> **Rationale:** Ein Dead-End-Zustand blockiert die gesamte Agenten-Kette und führt zu einem Systemstillstand. Dies ist ein fundamentaler logischer Fehler in "Conversation Routines" oder aufgabenorientierten Dialogsystemen. |
| | SM-1.2 | **Waisen-Zustände (Orphan States)** | **Prüffrage:** "Gibt es Zustände in der Definition, auf die keine Transition jemals hinführt? (d.h. sie sind logisch unerreichbar)" <br><br> **Rationale:** Identifiziert "toten Code" oder unvollständige Logikpfade im Systemdesign. Solche Zustände können auf Design-Fehler oder veraltete Logik hinweisen, die zu Wartungsproblemen führt. |
| | SM-1.3 | **Validierung der Fehler-Schleifen (Error Loops)** | **Prüffrage:** "Überprüfen Sie alle Transitions-Schleifen (z.B. AGENT_B -> AGENT_C -> AGENT_B). Stellt die Schleife sicher, dass der Ziel-Agent (AGENT_B) das für die Korrektur notwendige, strukturierte Fehler-Feedback (einen 'Failure Report' aus einem Datenvertrag) erhält?" <br><br> **Rationale:** Eine Schleife ohne korrigierendes Feedback ist eine potentielle Endlosschleife. Der Datenvertrag muss den Fehler semantisch transportieren, damit der Agent seinen Zustand ändern und die Schleife durchbrechen kann. |
| | SM-1.4 | **Vollständigkeit der Transitions-Bedingungen** | **Prüffrage:** "Prüfen Sie alle Zustände mit mehreren möglichen ausgehenden Transitionen. Decken die Bedingungen dieser Transitionen alle möglichen logischen Ausgaben des Agenten ab? Gibt es einen 'Default'- oder 'Catch-All'-Fehlerpfad?" <br><br> **Rationale:** Wenn ein Agent einen unerwarteten (aber validen) Status zurückgibt, für den keine Transition definiert ist, bleibt die Zustandsmaschine hängen. |
| | SM-1.5 | **Semantische Transitions-Validierung** | **Prüffrage:** "Prüfen Sie alle Transitions-Bedingungen. Sind sie binär und deterministisch (z.B. status == 'success') oder stützen sie sich auf eine mehrdeutige semantische Interpretation (z.B. review_sentiment == 'positive', summary_quality == 'high')?" <br><br> **Rationale:** LLMs sind von Natur aus nicht-deterministisch. Transitions-Logik, die auf einer semantischen Interpretation durch ein LLM (den Orchestrator) beruht, birgt ein hohes Risiko für "falsche" Zustandswechsel und unvorhersehbares Verhalten. |
| | SM-1.6 | **Ressourcen-Konkurrenz (Concurrency)** | **Prüffrage:** "Identifizieren Sie alle Zustände, die parallel ausgeführt werden könnten. Greifen diese Zustände oder die von ihnen aufgerufenen Agenten auf dieselbe Ressource (z.B. dieselbe Wissensdatei, denselben Datenvertrag) schreibend zu?" <br><br> **Rationale:** Obwohl es sich um ein Design-Audit handelt, müssen potentielle Race-Conditions oder Deadlocks, die durch das Design impliziert werden, frühzeitig erkannt werden. |
| **2. Validierung der Datenverträge (Data Contracts)** | DC-2.1 | **Producer-Consumer Kompatibilität** | **Prüffrage:** "Analysieren Sie alle 'Producer-Consumer'-Paare (z.B. Agent A's Output-Schema -> Agent B's Input-Schema). Ist das Output-Datenvertrag-Schema von A zu 100% syntaktisch UND semantisch mit dem Input-Datenvertrag-Schema von B kompatibel?" <br><br> **Rationale:** Dies ist die häufigste Fehlerquelle in verteilten Systemen. Ein "semantischer" Fehler liegt vor, wenn Feldnamen übereinstimmen (z.B. id), aber unterschiedliche Dinge bedeuten (z.B. user_id vs. session_id). |
| | DC-2.2 | **Daten-Vollständigkeits-Lücken (Gaps)** | **Prüffrage:** "Gibt es Daten, die von einem Agenten (Consumer) benötigt werden (gemäß seinem Prompt oder seiner Wissensbasis), aber von keinem vorherigen Agenten (Producer) erzeugt oder im Datenvertrag bereitgestellt werden?" <br><br> **Rationale:** Identifiziert semantische Lücken in der Datenpipeline. Der Consumer-Agent wird "verhungern" und seine Aufgabe nicht erfüllen können. |
| | DC-2.3 | **Daten-Überfluss (Data Bloat)** | **Prüffrage:** "Gibt es Daten, die von einem Agenten (Producer) erzeugt und im Datenvertrag übergeben werden, aber von keinem nachfolgenden Agenten (Consumer) jemals verwendet werden?" <br><br> **Rationale:** Dies ist zwar kein Fehler, der zum Absturz führt, aber ein Hinweis auf ein ineffizientes Design. Es verstößt gegen Prinzipien der Datenminimierung und kann die Leistung (z.B. Kontextfenster-Größe) beeinträchtigen. |
| | DC-2.4 | **Versions-Integrität und Provenienz** | **Prüffrage:** "Prüfen Sie alle Datenvertrags-Schemata (.yaml). Beziehen sie sich auf versionierte Schemata (z.B. schema_version: 1.2)? Wird diese Versionierung von allen Agenten (Producer/Consumer) konsistent referenziert?" <br><br> **Rationale:** Stellt sicher, dass das System gegen "Drift" bei Schema-Änderungen immun ist. Dies ist ein Kernprinzip der Provenienz, Reproduzierbarkeit und des Audit-Trails in regulierten Umgebungen. |
| | DC-2.5 | **API-Tool-Vertragsvalidierung** | **Prüffrage:** "Prüfen Sie die Prompts der Agenten, die Tools (APIs) verwenden. Stimmt die Definition des Tools im Prompt (Name, Parameter, Beschreibung) exakt mit dem Datenvertrag der tatsächlichen API-Implementierung überein?" <br><br> **Rationale:** LLM-Agenten sind bei der Tool-Nutzung stark auf die Prompt-Definition angewiesen. Eine Diskrepanz zwischen der im Prompt deklarierten "Signatur" und der realen API-Signatur führt unweigerlich zu Laufzeitfehlern. |
| **3. Validierung der Wissenskohärenz** | KB-3.1 | **Inter-File Widersprüche** | **Prüffrage:** "Gibt es widersprüchliche Regeln oder Constraints in den verschiedenen Wissensdatenbanken (.yaml-Dateien)? (z.B. agent_A_rules.yaml -> max_retries: 3, global_rules.yaml -> max_retries: 5)" <br><br> **Rationale:** Logische Inkonsistenzen in einer verteilten Wissensbasis führen zu unvorhersehbarem, nicht-deterministischem Verhalten, da unklar ist, welche Regel Priorität hat. |
| | KB-3.2 | **Semantische Ambiguität (Homonyme)** | **Prüffrage:** "Definieren verschiedene Agenten-Prompts oder Wissensdateien denselben Begriff (z.B. 'Fehlerpriorität', 'Review') auf unterschiedliche Weise? Verwendet ein Agent einen Begriff, der im Kontext eines anderen Agenten eine andere semantische Bedeutung hat?" <br><br> **Rationale:** Semantische Homonyme sind eine subtile Form der Wissenslücke ("Lexical Gap"). Das System scheint kohärent zu sein, aber die Agenten "reden aneinander vorbei". |
| | KB-3.3 | **Negations-Lücken (Negation Gaps)** | **Prüffrage:** "Analysieren Sie Regeln und Datenverträge auf 'Negation Gaps'. Versteht das System den Unterschied zwischen 'Task ist nicht aktiv' und 'Task ist inaktiv'? Wird ein Fehlerzustand durch das Fehlen eines Erfolgssignals oder das Vorhandensein eines Fehlersignals definiert? Ist dies konsistent?" <br><br> **Rationale:** Eine von der Forschung identifizierte hohe Risiko-Lücke, bei der Agenten logische Negationen falsch interpretieren oder das Fehlen eines Wertes nicht korrekt als Zustand interpretieren. |
| | KB-3.4 | **Sentiment-Lücken (Sentiment Gaps)** | **Prüffrage:** "Analysieren Sie alle Agenten-Prompts, die semantische Anweisungen geben (z.B. CODE_GEN soll sauberen Code schreiben). Definiert eine Wissensdatei, was sauber in diesem Kontext objektiv bedeutet (z.B. 'bestanden Linter', 'max 10 Zeilen pro Funktion')?" <br><br> **Rationale:** Eine "Sentiment Gap" tritt auf, wenn subjektive Begriffe (wie 'gut', 'sauber', 'hilfreich') ohne objektive Rubrik verwendet werden. Dies macht das Agentenverhalten unzuverlässig und nicht validierbar. |
| | KB-3.5 | **Hierarchie der Regeln (Precedence)** | **Prüffrage:** "Wenn Regeln widersprüchlich sind (siehe KB-3.1), gibt es eine explizite 'Precedence'-Regel (Vorrangsregel)? (z.B. 'Agenten-spezifische Regeln überschreiben globale Regeln')." <br><br> **Rationale:** Ohne eine klar definierte Hierarchie sind Regelkonflikte unlösbar und führen zu zufälligem Verhalten. |
| | KB-3.6 | **Analyse der Wissenslücken (Support)** | **Prüffrage:** "Analysieren Sie die Prompts und Wissensdateien. Welche Anfragen oder Szenarien, die für das Systemziel relevant sind, werden nicht durch das Wissen abgedeckt?" (z.B. "Wie geht das System mit einem CODE_GEN-Antrag für eine nicht unterstützte Programmiersprache um?"). <br><br> **Rationale:** Dies ist eine proaktive Suche nach Wissenslücken, ähnlich wie Support-Teams Wissensdatenbank-Lücken durch die Analyse von Tickets finden. |
| **4. Identifizierung von Blind Spots (Red Team)** | BS-4.1 | **Unabgedeckte Kernprozesse** | **Prüffrage:** "Welche geschäftskritischen, nicht-funktionalen Kernprozesse (z.B. 'Security Auditing', 'Kosten-Tracking', 'Daten-Archivierung', 'Logging-Strategie', 'Benutzer-Feedback-Schleife') werden von der aktuellen State Machine und den Agenten-Definitionen nicht explizit abgedeckt?" <br><br> **Rationale:** Findet die "Unknown Unknowns" im Systemdesign. Oft wird die "Happy Path"-Logik entworfen, aber die Governance- und Support-Prozesse werden vergessen. |
| | BS-4.2 | **Implizite Annahmen (Execution Engine)** | **Prüffrage:** "Welche Annahmen über die 'Execution Engine' (z.B. 'Durable Execution' [Zustand überlebt Neustart], 'Atomicity' von Operationen, 'Zustands-Persistenz' nach einem Absturz) werden stillschweigend vorausgesetzt, aber nicht explizit definiert?" <br><br> **Rationale:** Basierend auf den Fallstricken monolithischer Architekturen müssen diese Annahamen explizit gemacht werden. Ein Design, das Atomizität annimmt, aber auf einer nicht-atomaren Engine läuft, wird fehlschlagen. |
| | BS-4.3 | **Angriffsvektor: Kontext-Vermischung (Prompt Injection)** | **Prüffrage:** "Analysieren Sie alle Punkte, an denen unstrukturierte Benutzerdaten (z.B. ein Projekt-Briefing) mit System-Prompts oder Wissens-Dateien in Kontakt kommen. Könnte ein Benutzer-Input Anweisungen enthalten, die den Orchestrator oder einen Spezialisten-Agenten manipulieren?" <br><br> **Rationale:** Dies ist ein primärer Angriffsvektor für Red Teams. Das Design muss eine strikte Trennung von Anweisung (Prompt) und Daten (Input) validieren. |
| | BS-4.4 | **Risiko der "Supervisor-Überlappung"** | **Prüffrage:** "Analysieren Sie die Prompts und Fähigkeiten der Spezialisten-Agenten. Gibt es eine signifikante Überlappung in ihren Verantwortlichkeiten? (z.B. könnten sowohl PLANNING als auch CODE_GEN versuchen, Dateisystem-Operationen durchzuführen?)" <br><br> **Rationale:** Eine bewährte Praxis bei der Skalierung von Multi-Agenten-Systemen ist die Vermeidung von Agenten-Überlappungen, da dies den Orchestrator verwirrt und zu Leistungseinbußen führt. |
| | BS-4.5 | **Agenten-Kollusion (Collusion)** | **Prüffrage:** "Gibt es einen Pfad, bei dem zwei oder mehr Agenten in einer Schleife (siehe SM-1.3) zusammenarbeiten könnten, um eine globale Systemregel (z.B. max_retries) zu umgehen, indem sie den Fehler zwischen sich hin und her schieben, ohne ihn an den Orchestrator zu eskalieren?" <br><br> **Rationale:** Ein fortgeschrittener "Red Team"-Gedanke, bei dem das emergente Verhalten von zwei Agenten eine im globalen Wissen definierte Leitplanke verletzt. |
